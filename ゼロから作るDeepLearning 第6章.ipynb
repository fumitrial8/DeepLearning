{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "def affine(z, W, b):\n",
    "    return np.dot(z, W) + b\n",
    "\n",
    "\n",
    "def relu(u):\n",
    "    return np.log(1+np.exp(u))\n",
    "#     return np.maximum(0,u)\n",
    "\n",
    "\n",
    "def softmax(u):\n",
    "    max_u = np.max(u, axis=1, keepdims=True)\n",
    "    exp_u = np.exp(u-max_u)\n",
    "    return exp_u/np.sum(exp_u, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    return -np.sum(t * np.log(np.maximum(y, 1e-7)))/y.shape[0]\n",
    "\n",
    "\n",
    "def softmax_cross_entropy_error_back(y,t):\n",
    "    return (y-t)/y.shape[0]\n",
    "\n",
    "\n",
    "def relu_back(dz, u):\n",
    "    return dz*np.where(u > 0, 1,0)\n",
    "\n",
    "\n",
    "def affine_back(du ,z ,W ,b):\n",
    "    dz = np.dot(du, W.T)\n",
    "    dW = np.dot(z.T, du)\n",
    "    db = np.dot(np.ones(z.shape[0]).T, du)\n",
    "    return dz, dW, db\n",
    "\n",
    "def learn(x, t, W1, b1, W2, b2, W3, b3, lr):\n",
    "    u1 = affine(x,W1,b1)\n",
    "    z1 = relu(u1)\n",
    "    u2 = affine(z1, W2, b2)\n",
    "    z2 = relu(u2)\n",
    "    u3 = affine(z2, W3, b3)\n",
    "    y = softmax(u3)\n",
    "    \n",
    "    dy = softmax_cross_entropy_error_back(y,t)\n",
    "    dz2, dW3, db3 = affine_back(dy, z2, W3 , b3)\n",
    "    du2 = relu_back(dz2, u2)\n",
    "    dz1, dW2, db2 = affine_back(du2, z1, W2, b2)\n",
    "    du1 = relu_back(dz1, u1)\n",
    "    dx, dW1, db1 = affine_back(du1, x, W1, b1)\n",
    "    W1 = W1 - lr * dW1\n",
    "    b1 = b1 - lr * db1\n",
    "    W2 = W2 - lr * dW2\n",
    "    b2 = b2 - lr * db2\n",
    "    W3 = W3 - lr * dW3\n",
    "    b3 = b3 - lr * db3\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "# momentum法で使う、重みの履歴を保存する部分\n",
    "def momentum_decay(history, dW, lr, momentum=0.9):\n",
    "    history = momentum * history - lr * dW\n",
    "    return history\n",
    "\n",
    "# momentum法の学習メソッド \n",
    "def Momentum_learn(x, t, W1, b1, W2, b2, W3, b3, history_W1, history_W2, history_W3, lr, momentum=0.9):\n",
    "    u1 = affine(x,W1,b1)\n",
    "    z1 = relu(u1)\n",
    "    u2 = affine(z1, W2, b2)\n",
    "    z2 = relu(u2)\n",
    "    u3 = affine(z2, W3, b3)\n",
    "    y = softmax(u3)\n",
    "    \n",
    "    dy = softmax_cross_entropy_error_back(y,t)\n",
    "    dz2, dW3, db3 = affine_back(dy, z2, W3 , b3)\n",
    "    du2 = relu_back(dz2, u2)\n",
    "    dz1, dW2, db2 = affine_back(du2, z1, W2, b2)\n",
    "    du1 = relu_back(dz1, u1)\n",
    "    dx, dW1, db1 = affine_back(du1, x, W1, b1)\n",
    "    history_W1 = momentum_decay(history_W1, dW1, lr)\n",
    "    history_W2 = momentum_decay(history_W2, dW2, lr)\n",
    "    history_W3 = momentum_decay(history_W3, dW3, lr)\n",
    "    W1 = W1 - lr * dW1\n",
    "    b1 = b1 - lr * db1\n",
    "    W2 = W2 - lr * dW2\n",
    "    b2 = b2 - lr * db2\n",
    "    W3 = W3 - lr * dW3\n",
    "    b3 = b3 - lr * db3\n",
    "    return W1, b1, W2, b2, W3, b3, history_W1, history_W2, history_W3\n",
    "\n",
    "# AdaGrad法で使う、重みの履歴を保存する部分\n",
    "def weight_decay(history, w):\n",
    "    history += np.square(w)\n",
    "    return history\n",
    "\n",
    "# AdaGrad法の学習メソッド \n",
    "def AdaGrad_learn(x, t, W1, b1, W2, b2, W3, b3, history_W1, history_W2, history_W3, lr):\n",
    "    u1 = affine(x,W1,b1)\n",
    "    z1 = relu(u1)\n",
    "    u2 = affine(z1, W2, b2)\n",
    "    z2 = relu(u2)\n",
    "    u3 = affine(z2, W3, b3)\n",
    "    y = softmax(u3)\n",
    "\n",
    "    dy = softmax_cross_entropy_error_back(y,t)\n",
    "    dz2, dW3, db3 = affine_back(dy, z2, W3 , b3)\n",
    "    du2 = relu_back(dz2, u2)\n",
    "    dz1, dW2, db2 = affine_back(du2, z1, W2, b2)\n",
    "    du1 = relu_back(dz1, u1)\n",
    "    dx, dW1, db1 = affine_back(du1, x, W1, b1)\n",
    "    history_W1 = weight_decay(history_W1, dW1)\n",
    "    history_W2 = weight_decay(history_W2, dW2)\n",
    "    history_W3 = weight_decay(history_W3, dW3)\n",
    "\n",
    "    W1 = W1 - lr * dW1/ (np.sqrt(history_W1)+1e-7)\n",
    "    b1 = b1 - lr * db1\n",
    "    W2 = W2 - lr * dW2/ (np.sqrt(history_W2)+1e-7)\n",
    "    b2 = b2 - lr * db2\n",
    "    W3 = W3 - lr * dW3/ (np.sqrt(history_W3)+1e-7)\n",
    "    b3 = b3 - lr * db3\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3, history_W1, history_W2, history_W3\n",
    "\n",
    "\n",
    "def predict(x, W1, b1, W2, b2, W3, b3):\n",
    "    u1 = affine(x, W1, b1)\n",
    "    z1 = relu(u1)\n",
    "    u2 = affine(z1, W2, b2)\n",
    "    z2 = relu(u2)\n",
    "    u3 = affine(z2, W3, b3)\n",
    "    y = softmax(u3)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "# 正解率\n",
    "def accuracy_rate(y, t):\n",
    "    max_y = np.argmax(y, axis=1)\n",
    "    max_t = np.argmax(t, axis=1)\n",
    "    return np.sum(max_y == max_t)/y.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "# keras.datasetsのmnistデータを使った学習の実装\n",
    "# 書籍ではデータをダウンロードして説明していますが、こちらの方がダウンロードの手間がなくて簡単です\n",
    "\n",
    "(X_train, b_train), (X_test, b_test) = mnist.load_data()\n",
    "\n",
    "# 書籍のデータセットのnp.shapeが(60000, 576)であり、kerasのデータセットは(数万, 28,28)なので、整形する必要あ\n",
    "x_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
    "x_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n",
    "t_train = np.zeros((b_train.shape[0], 10))\n",
    "t_test = np.zeros((b_test.shape[0], 10))\n",
    "\n",
    "\n",
    "for index in range(len(b_train)):\n",
    "    t_train[index, int(b_train[index])] = 1\n",
    "for index in range(len(b_test)):\n",
    "    t_test[index, int(b_test[index])] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD的手法で学習を進める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08026386  0.10248935  0.07929109 ...  0.02781289 -0.0886679\n",
      "   0.06104786]\n",
      " [ 0.00041565  0.10930881  0.10617994 ... -0.04557829 -0.08723505\n",
      "  -0.04816912]\n",
      " [-0.02218244 -0.00056607 -0.03774787 ... -0.05218316 -0.02957538\n",
      "  -0.06802625]\n",
      " ...\n",
      " [ 0.04726429 -0.08883908 -0.04127646 ... -0.00674194  0.07857021\n",
      "  -0.008147  ]\n",
      " [ 0.09798906  0.08489334  0.07804969 ...  0.06872335  0.08430461\n",
      "  -0.07813033]\n",
      " [ 0.07454521 -0.01339799  0.01067081 ... -0.01921345 -0.02373026\n",
      "   0.10860668]]\n",
      "  0 train_rate= 10.26% test_rate= 10.29% train_err= 2.31935 test_err= 2.31875\n",
      "  1 train_rate= 92.11% test_rate= 92.34% train_err= 0.25767 test_err= 0.24383\n",
      "  2 train_rate= 95.31% test_rate= 95.14% train_err= 0.15365 test_err= 0.15337\n",
      "  3 train_rate= 96.65% test_rate= 96.17% train_err= 0.10769 test_err= 0.12189\n",
      "  4 train_rate= 97.30% test_rate= 96.63% train_err= 0.08490 test_err= 0.10930\n",
      "  5 train_rate= 97.67% test_rate= 96.97% train_err= 0.07300 test_err= 0.10179\n",
      "  6 train_rate= 97.96% test_rate= 96.99% train_err= 0.06411 test_err= 0.09949\n",
      "  7 train_rate= 98.10% test_rate= 97.19% train_err= 0.05914 test_err= 0.10312\n",
      "  8 train_rate= 98.08% test_rate= 97.03% train_err= 0.05812 test_err= 0.11094\n",
      "  9 train_rate= 98.11% test_rate= 97.04% train_err= 0.05737 test_err= 0.11720\n",
      " 10 train_rate= 98.25% test_rate= 97.03% train_err= 0.05283 test_err= 0.11610\n",
      " 11 train_rate= 98.23% test_rate= 97.03% train_err= 0.05349 test_err= 0.12585\n",
      " 12 train_rate= 98.25% test_rate= 96.78% train_err= 0.05354 test_err= 0.13127\n",
      " 13 train_rate= 98.23% test_rate= 97.05% train_err= 0.05222 test_err= 0.13504\n",
      " 14 train_rate= 98.12% test_rate= 96.84% train_err= 0.05732 test_err= 0.14605\n",
      " 15 train_rate= 98.75% test_rate= 97.15% train_err= 0.03881 test_err= 0.13478\n",
      " 16 train_rate= 98.25% test_rate= 96.85% train_err= 0.05564 test_err= 0.14209\n",
      " 17 train_rate= 98.45% test_rate= 96.83% train_err= 0.04763 test_err= 0.14637\n",
      " 18 train_rate= 98.95% test_rate= 97.29% train_err= 0.03340 test_err= 0.13536\n",
      " 19 train_rate= 98.90% test_rate= 97.17% train_err= 0.03573 test_err= 0.14143\n",
      " 20 train_rate= 99.24% test_rate= 97.41% train_err= 0.02242 test_err= 0.13047\n",
      " 21 train_rate= 98.95% test_rate= 97.10% train_err= 0.03327 test_err= 0.14821\n",
      " 22 train_rate= 99.09% test_rate= 97.32% train_err= 0.02917 test_err= 0.15229\n",
      " 23 train_rate= 92.85% test_rate= 92.17% train_err= 0.23289 test_err= 0.25591\n",
      " 24 train_rate= 96.54% test_rate= 95.88% train_err= 0.11294 test_err= 0.14735\n",
      " 25 train_rate= 97.28% test_rate= 96.25% train_err= 0.08649 test_err= 0.13244\n",
      " 26 train_rate= 97.72% test_rate= 96.48% train_err= 0.06984 test_err= 0.12549\n",
      " 27 train_rate= 97.85% test_rate= 96.66% train_err= 0.06355 test_err= 0.12590\n",
      " 28 train_rate= 98.18% test_rate= 96.71% train_err= 0.05532 test_err= 0.12301\n",
      " 29 train_rate= 98.43% test_rate= 96.88% train_err= 0.04693 test_err= 0.12700\n",
      " 30 train_rate= 98.66% test_rate= 97.09% train_err= 0.04044 test_err= 0.12491\n",
      " 31 train_rate= 98.88% test_rate= 97.10% train_err= 0.03502 test_err= 0.12941\n",
      " 32 train_rate= 98.70% test_rate= 96.90% train_err= 0.03773 test_err= 0.13908\n",
      " 33 train_rate= 98.86% test_rate= 97.00% train_err= 0.03470 test_err= 0.13731\n",
      " 34 train_rate= 98.98% test_rate= 97.00% train_err= 0.02926 test_err= 0.14154\n",
      " 35 train_rate= 99.00% test_rate= 97.08% train_err= 0.02934 test_err= 0.14344\n",
      " 36 train_rate= 98.86% test_rate= 96.77% train_err= 0.03517 test_err= 0.15996\n",
      " 37 train_rate= 99.09% test_rate= 97.18% train_err= 0.02720 test_err= 0.14761\n",
      " 38 train_rate= 98.96% test_rate= 97.08% train_err= 0.03030 test_err= 0.15581\n",
      " 39 train_rate= 99.19% test_rate= 97.18% train_err= 0.02392 test_err= 0.15026\n",
      " 40 train_rate= 99.08% test_rate= 97.12% train_err= 0.02848 test_err= 0.15756\n",
      " 41 train_rate= 99.25% test_rate= 97.39% train_err= 0.02262 test_err= 0.14253\n",
      " 42 train_rate= 99.48% test_rate= 97.43% train_err= 0.01544 test_err= 0.14405\n",
      " 43 train_rate= 99.42% test_rate= 97.09% train_err= 0.01727 test_err= 0.14966\n",
      " 44 train_rate= 99.39% test_rate= 97.04% train_err= 0.01725 test_err= 0.15197\n",
      " 45 train_rate= 99.41% test_rate= 97.21% train_err= 0.01772 test_err= 0.15760\n",
      " 46 train_rate= 99.59% test_rate= 97.32% train_err= 0.01362 test_err= 0.14859\n",
      " 47 train_rate= 99.67% test_rate= 97.52% train_err= 0.00997 test_err= 0.14807\n",
      " 48 train_rate= 99.53% test_rate= 97.28% train_err= 0.01388 test_err= 0.16432\n",
      " 49 train_rate= 99.60% test_rate= 97.25% train_err= 0.01207 test_err= 0.15962\n",
      " 50 train_rate= 99.59% test_rate= 97.37% train_err= 0.01285 test_err= 0.16254\n"
     ]
    }
   ],
   "source": [
    "nx_train = x_train/255\n",
    "nx_test = x_test/255\n",
    "\n",
    "d0 = nx_train.shape[1]\n",
    "d1 = 100\n",
    "d2 = 50\n",
    "d3 = 10\n",
    "\n",
    "np.random.seed(8)\n",
    "W1 = np.random.rand(d0, d1) * 0.2 -0.1\n",
    "W2 = np.random.rand(d1, d2) * 0.2 -0.1\n",
    "W3 = np.random.rand(d2, d3) * 0.2 -0.1\n",
    "\n",
    "b1 = np.zeros(d1)\n",
    "b2 = np.zeros(d2)\n",
    "b3 = np.zeros(d3)\n",
    "lr = 0.5\n",
    "\n",
    "batch_size = 100\n",
    "epoch = 50\n",
    "y_train = predict(nx_train, W1, b1, W2, b2, W3, b3)\n",
    "y_test = predict(nx_test, W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "\n",
    "train_rate, train_err = accuracy_rate(y_train, t_train), cross_entropy_error(y_train, t_train)\n",
    "test_rate, test_err = accuracy_rate(y_test, t_test), cross_entropy_error(y_test, t_test)\n",
    "print(\"{0:3d} train_rate={1:6.2f}% test_rate={2:6.2f}% train_err={3:8.5f} test_err={4:8.5f}\".format((0), train_rate*100, test_rate*100, train_err, test_err))\n",
    "for i in range(epoch):\n",
    "    for j in range(0,nx_train.shape[0], batch_size):\n",
    "        W1, b1, W2, b2, W3, b3 = learn(nx_train[j:j+batch_size], t_train[j:j+batch_size], W1, b1, W2, b2, W3, b3, lr)\n",
    "    y_train = predict(nx_train, W1, b1, W2, b2, W3, b3)\n",
    "    y_test = predict(nx_test, W1, b1, W2, b2, W3, b3)\n",
    "    train_rate, train_err = accuracy_rate(y_train, t_train), cross_entropy_error(y_train, t_train)\n",
    "    test_rate, test_err = accuracy_rate(y_test, t_test), cross_entropy_error(y_test, t_test)\n",
    "    print(\"{0:3d} train_rate={1:6.2f}% test_rate={2:6.2f}% train_err={3:8.5f} test_err={4:8.5f}\".format((i+1), train_rate*100, test_rate*100, train_err, test_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad法を用いて学習を進める\n",
    "* 勾配の履歴の二乗和を記憶し、学習を進めるに連れて学習率を下げていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 train_rate= 10.26% test_rate= 10.29% train_err= 2.31935 test_err= 2.31875\n",
      "  1 train_rate= 28.51% test_rate= 29.30% train_err= 2.05276 test_err= 2.06062\n",
      "  6 train_rate= 40.37% test_rate= 40.70% train_err= 1.85213 test_err= 1.85329\n",
      " 11 train_rate= 42.88% test_rate= 43.03% train_err= 1.77157 test_err= 1.77105\n",
      " 16 train_rate= 44.18% test_rate= 44.30% train_err= 1.72209 test_err= 1.72067\n",
      " 21 train_rate= 45.10% test_rate= 45.26% train_err= 1.68717 test_err= 1.68519\n",
      " 26 train_rate= 45.85% test_rate= 45.97% train_err= 1.66059 test_err= 1.65822\n",
      " 31 train_rate= 46.45% test_rate= 46.60% train_err= 1.63935 test_err= 1.63668\n",
      " 36 train_rate= 46.99% test_rate= 47.30% train_err= 1.62176 test_err= 1.61888\n",
      " 41 train_rate= 47.44% test_rate= 47.78% train_err= 1.60684 test_err= 1.60378\n",
      " 46 train_rate= 47.85% test_rate= 48.19% train_err= 1.59393 test_err= 1.59073\n",
      " 51 train_rate= 48.16% test_rate= 48.75% train_err= 1.58258 test_err= 1.57927\n",
      " 56 train_rate= 48.54% test_rate= 49.00% train_err= 1.57248 test_err= 1.56908\n",
      " 61 train_rate= 48.87% test_rate= 49.33% train_err= 1.56340 test_err= 1.55993\n",
      " 66 train_rate= 49.16% test_rate= 49.69% train_err= 1.55516 test_err= 1.55163\n",
      " 71 train_rate= 49.44% test_rate= 49.99% train_err= 1.54763 test_err= 1.54405\n",
      " 76 train_rate= 49.69% test_rate= 50.34% train_err= 1.54071 test_err= 1.53709\n",
      " 81 train_rate= 49.95% test_rate= 50.63% train_err= 1.53431 test_err= 1.53066\n",
      " 86 train_rate= 50.16% test_rate= 50.92% train_err= 1.52836 test_err= 1.52469\n",
      " 91 train_rate= 50.35% test_rate= 51.01% train_err= 1.52281 test_err= 1.51912\n",
      " 96 train_rate= 50.53% test_rate= 51.12% train_err= 1.51761 test_err= 1.51390\n",
      "101 train_rate= 50.68% test_rate= 51.16% train_err= 1.51273 test_err= 1.50901\n",
      "106 train_rate= 50.87% test_rate= 51.27% train_err= 1.50812 test_err= 1.50439\n",
      "111 train_rate= 51.03% test_rate= 51.43% train_err= 1.50377 test_err= 1.50004\n",
      "116 train_rate= 51.21% test_rate= 51.51% train_err= 1.49964 test_err= 1.49591\n",
      "121 train_rate= 51.36% test_rate= 51.67% train_err= 1.49572 test_err= 1.49199\n",
      "126 train_rate= 51.52% test_rate= 51.75% train_err= 1.49199 test_err= 1.48826\n",
      "131 train_rate= 51.64% test_rate= 51.84% train_err= 1.48843 test_err= 1.48471\n",
      "136 train_rate= 51.81% test_rate= 51.97% train_err= 1.48503 test_err= 1.48131\n",
      "141 train_rate= 51.91% test_rate= 52.04% train_err= 1.48178 test_err= 1.47807\n",
      "146 train_rate= 52.05% test_rate= 52.17% train_err= 1.47866 test_err= 1.47496\n",
      "151 train_rate= 52.15% test_rate= 52.22% train_err= 1.47566 test_err= 1.47197\n",
      "156 train_rate= 52.26% test_rate= 52.24% train_err= 1.47278 test_err= 1.46910\n",
      "161 train_rate= 52.34% test_rate= 52.35% train_err= 1.47001 test_err= 1.46635\n",
      "166 train_rate= 52.45% test_rate= 52.48% train_err= 1.46734 test_err= 1.46369\n",
      "171 train_rate= 52.54% test_rate= 52.56% train_err= 1.46476 test_err= 1.46113\n",
      "176 train_rate= 52.66% test_rate= 52.65% train_err= 1.46228 test_err= 1.45866\n",
      "181 train_rate= 52.78% test_rate= 52.73% train_err= 1.45987 test_err= 1.45627\n",
      "186 train_rate= 52.87% test_rate= 52.79% train_err= 1.45755 test_err= 1.45396\n",
      "191 train_rate= 52.95% test_rate= 52.95% train_err= 1.45529 test_err= 1.45172\n",
      "196 train_rate= 52.99% test_rate= 52.99% train_err= 1.45311 test_err= 1.44956\n",
      "201 train_rate= 53.06% test_rate= 53.08% train_err= 1.45099 test_err= 1.44746\n",
      "206 train_rate= 53.14% test_rate= 53.19% train_err= 1.44894 test_err= 1.44542\n",
      "211 train_rate= 53.21% test_rate= 53.19% train_err= 1.44694 test_err= 1.44344\n",
      "216 train_rate= 53.29% test_rate= 53.30% train_err= 1.44500 test_err= 1.44152\n",
      "221 train_rate= 53.36% test_rate= 53.35% train_err= 1.44311 test_err= 1.43965\n",
      "226 train_rate= 53.42% test_rate= 53.40% train_err= 1.44128 test_err= 1.43783\n",
      "231 train_rate= 53.48% test_rate= 53.48% train_err= 1.43949 test_err= 1.43606\n",
      "236 train_rate= 53.53% test_rate= 53.54% train_err= 1.43774 test_err= 1.43434\n",
      "241 train_rate= 53.60% test_rate= 53.65% train_err= 1.43605 test_err= 1.43266\n",
      "246 train_rate= 53.67% test_rate= 53.66% train_err= 1.43439 test_err= 1.43102\n",
      "251 train_rate= 53.73% test_rate= 53.74% train_err= 1.43277 test_err= 1.42942\n",
      "256 train_rate= 53.79% test_rate= 53.83% train_err= 1.43119 test_err= 1.42786\n",
      "261 train_rate= 53.85% test_rate= 53.88% train_err= 1.42965 test_err= 1.42634\n",
      "266 train_rate= 53.93% test_rate= 53.94% train_err= 1.42814 test_err= 1.42485\n",
      "271 train_rate= 53.99% test_rate= 54.02% train_err= 1.42667 test_err= 1.42340\n",
      "276 train_rate= 54.06% test_rate= 54.05% train_err= 1.42523 test_err= 1.42198\n",
      "281 train_rate= 54.12% test_rate= 54.08% train_err= 1.42382 test_err= 1.42059\n",
      "286 train_rate= 54.15% test_rate= 54.16% train_err= 1.42244 test_err= 1.41923\n",
      "291 train_rate= 54.23% test_rate= 54.25% train_err= 1.42109 test_err= 1.41789\n",
      "296 train_rate= 54.28% test_rate= 54.30% train_err= 1.41976 test_err= 1.41659\n",
      "301 train_rate= 54.32% test_rate= 54.32% train_err= 1.41847 test_err= 1.41531\n",
      "306 train_rate= 54.38% test_rate= 54.31% train_err= 1.41720 test_err= 1.41406\n",
      "311 train_rate= 54.42% test_rate= 54.34% train_err= 1.41595 test_err= 1.41283\n",
      "316 train_rate= 54.44% test_rate= 54.43% train_err= 1.41473 test_err= 1.41163\n",
      "321 train_rate= 54.50% test_rate= 54.46% train_err= 1.41353 test_err= 1.41045\n",
      "326 train_rate= 54.55% test_rate= 54.52% train_err= 1.41235 test_err= 1.40929\n",
      "331 train_rate= 54.59% test_rate= 54.54% train_err= 1.41120 test_err= 1.40816\n",
      "336 train_rate= 54.62% test_rate= 54.57% train_err= 1.41006 test_err= 1.40704\n",
      "341 train_rate= 54.66% test_rate= 54.62% train_err= 1.40895 test_err= 1.40595\n",
      "346 train_rate= 54.69% test_rate= 54.61% train_err= 1.40785 test_err= 1.40487\n",
      "351 train_rate= 54.73% test_rate= 54.68% train_err= 1.40678 test_err= 1.40381\n",
      "356 train_rate= 54.77% test_rate= 54.69% train_err= 1.40572 test_err= 1.40278\n",
      "361 train_rate= 54.80% test_rate= 54.75% train_err= 1.40468 test_err= 1.40175\n",
      "366 train_rate= 54.83% test_rate= 54.79% train_err= 1.40366 test_err= 1.40075\n",
      "371 train_rate= 54.87% test_rate= 54.80% train_err= 1.40266 test_err= 1.39976\n",
      "376 train_rate= 54.91% test_rate= 54.82% train_err= 1.40167 test_err= 1.39879\n",
      "381 train_rate= 54.94% test_rate= 54.87% train_err= 1.40069 test_err= 1.39784\n",
      "386 train_rate= 54.96% test_rate= 54.88% train_err= 1.39974 test_err= 1.39690\n",
      "391 train_rate= 54.99% test_rate= 54.91% train_err= 1.39879 test_err= 1.39597\n",
      "396 train_rate= 55.02% test_rate= 54.93% train_err= 1.39786 test_err= 1.39506\n",
      "401 train_rate= 55.05% test_rate= 54.96% train_err= 1.39695 test_err= 1.39417\n",
      "406 train_rate= 55.09% test_rate= 55.00% train_err= 1.39605 test_err= 1.39328\n",
      "411 train_rate= 55.11% test_rate= 55.02% train_err= 1.39516 test_err= 1.39241\n",
      "416 train_rate= 55.14% test_rate= 55.07% train_err= 1.39429 test_err= 1.39156\n",
      "421 train_rate= 55.16% test_rate= 55.12% train_err= 1.39343 test_err= 1.39071\n",
      "426 train_rate= 55.18% test_rate= 55.12% train_err= 1.39258 test_err= 1.38988\n",
      "431 train_rate= 55.22% test_rate= 55.14% train_err= 1.39174 test_err= 1.38906\n",
      "436 train_rate= 55.25% test_rate= 55.20% train_err= 1.39092 test_err= 1.38825\n",
      "441 train_rate= 55.28% test_rate= 55.23% train_err= 1.39010 test_err= 1.38746\n",
      "446 train_rate= 55.31% test_rate= 55.30% train_err= 1.38930 test_err= 1.38667\n",
      "451 train_rate= 55.34% test_rate= 55.30% train_err= 1.38851 test_err= 1.38590\n",
      "456 train_rate= 55.37% test_rate= 55.36% train_err= 1.38773 test_err= 1.38513\n",
      "461 train_rate= 55.37% test_rate= 55.38% train_err= 1.38696 test_err= 1.38438\n",
      "466 train_rate= 55.39% test_rate= 55.40% train_err= 1.38620 test_err= 1.38363\n",
      "471 train_rate= 55.42% test_rate= 55.44% train_err= 1.38545 test_err= 1.38290\n",
      "476 train_rate= 55.44% test_rate= 55.44% train_err= 1.38470 test_err= 1.38217\n",
      "481 train_rate= 55.47% test_rate= 55.44% train_err= 1.38397 test_err= 1.38146\n",
      "486 train_rate= 55.50% test_rate= 55.46% train_err= 1.38325 test_err= 1.38075\n",
      "491 train_rate= 55.52% test_rate= 55.51% train_err= 1.38254 test_err= 1.38005\n",
      "496 train_rate= 55.54% test_rate= 55.54% train_err= 1.38183 test_err= 1.37936\n"
     ]
    }
   ],
   "source": [
    "# 画像のスケールを0-1に正規化\n",
    "nx_train = x_train/255\n",
    "nx_test = x_test/255\n",
    "\n",
    "# dk 第k層の次元を設定 \n",
    "d0 = nx_train.shape[1]\n",
    "d1 = 100\n",
    "d2 = 50\n",
    "d3 = 10\n",
    "\n",
    "np.random.seed(8)\n",
    "W1 = np.random.rand(d0, d1) * 0.2 -0.1\n",
    "W2 = np.random.rand(d1, d2) * 0.2 -0.1\n",
    "W3 = np.random.rand(d2, d3) * 0.2 -0.1\n",
    "\n",
    "# historyにこれまでの学習を記憶させるために、重みと同じ形状の零行列で初期化\n",
    "history_W1 = np.zeros_like(W1)\n",
    "history_W2 = np.zeros_like(W2)\n",
    "history_W3 = np.zeros_like(W3)\n",
    "b1 = np.zeros(d1)\n",
    "b2 = np.zeros(d2)\n",
    "b3 = np.zeros(d3)\n",
    "lr = 0.9\n",
    "\n",
    "batch_size = 100\n",
    "epoch = 500\n",
    "y_train = predict(nx_train, W1, b1, W2, b2, W3, b3)\n",
    "y_test = predict(nx_test, W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "# メソッドの実装はページ上部を参照\n",
    "history_W1 = weight_decay(history_W1, W1)\n",
    "history_W2 = weight_decay(history_W2, W2)\n",
    "history_W3 = weight_decay(history_W3, W3)\n",
    "\n",
    "train_rate, train_err = accuracy_rate(y_train, t_train), cross_entropy_error(y_train, t_train)\n",
    "test_rate, test_err = accuracy_rate(y_test, t_test), cross_entropy_error(y_test, t_test)\n",
    "print(\"{0:3d} train_rate={1:6.2f}% test_rate={2:6.2f}% train_err={3:8.5f} test_err={4:8.5f}\".format((0), train_rate*100, test_rate*100, train_err, test_err))\n",
    "for i in range(epoch):\n",
    "    for j in range(0,nx_train.shape[0], batch_size):\n",
    "        W1, b1, W2, b2, W3, b3, history_W1, history_W2, history_W3 = AdaGrad_learn(nx_train[j:j+batch_size], t_train[j:j+batch_size], W1, b1, W2, b2, W3, b3, history_W1, history_W2, history_W3, lr)\n",
    "    y_train = predict(nx_train, W1, b1, W2, b2, W3, b3)\n",
    "    y_test = predict(nx_test, W1, b1, W2, b2, W3, b3)\n",
    "    train_rate, train_err = accuracy_rate(y_train, t_train), cross_entropy_error(y_train, t_train)\n",
    "    test_rate, test_err = accuracy_rate(y_test, t_test), cross_entropy_error(y_test, t_test)\n",
    "    if i%5 ==0:\n",
    "        print(\"{0:3d} train_rate={1:6.2f}% test_rate={2:6.2f}% train_err={3:8.5f} test_err={4:8.5f}\".format((i+1), train_rate*100, test_rate*100, train_err, test_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum法で学習\n",
    "* 学習エポック間の勾配の変化量に応じて、勾配のベクトルの変化を緩やかにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 train_rate= 10.26% test_rate= 10.29% train_err= 2.31935 test_err= 2.31875\n",
      "  1 train_rate= 91.65% test_rate= 92.23% train_err= 0.26123 test_err= 0.24636\n",
      "  2 train_rate= 94.27% test_rate= 94.40% train_err= 0.18031 test_err= 0.18563\n",
      "  3 train_rate= 96.41% test_rate= 96.15% train_err= 0.11143 test_err= 0.12923\n",
      "  4 train_rate= 97.09% test_rate= 96.52% train_err= 0.09020 test_err= 0.11592\n",
      "  5 train_rate= 97.50% test_rate= 96.79% train_err= 0.07624 test_err= 0.10875\n",
      "  6 train_rate= 97.65% test_rate= 96.91% train_err= 0.07185 test_err= 0.11039\n",
      "  7 train_rate= 98.05% test_rate= 97.13% train_err= 0.05992 test_err= 0.10503\n",
      "  8 train_rate= 98.13% test_rate= 97.03% train_err= 0.05823 test_err= 0.11039\n",
      "  9 train_rate= 98.00% test_rate= 96.91% train_err= 0.06050 test_err= 0.11906\n",
      " 10 train_rate= 98.39% test_rate= 97.28% train_err= 0.04987 test_err= 0.11197\n",
      " 11 train_rate= 97.69% test_rate= 96.52% train_err= 0.07051 test_err= 0.14725\n",
      " 12 train_rate= 98.47% test_rate= 97.07% train_err= 0.04649 test_err= 0.12238\n",
      " 13 train_rate= 98.28% test_rate= 96.57% train_err= 0.05164 test_err= 0.13701\n",
      " 14 train_rate= 98.69% test_rate= 97.21% train_err= 0.03987 test_err= 0.12614\n",
      " 15 train_rate= 98.89% test_rate= 97.26% train_err= 0.03426 test_err= 0.12301\n",
      " 16 train_rate= 98.94% test_rate= 97.33% train_err= 0.03167 test_err= 0.12780\n",
      " 17 train_rate= 98.83% test_rate= 97.29% train_err= 0.03491 test_err= 0.13702\n",
      " 18 train_rate= 98.91% test_rate= 97.19% train_err= 0.03310 test_err= 0.13480\n",
      " 19 train_rate= 99.21% test_rate= 97.28% train_err= 0.02366 test_err= 0.12799\n",
      " 20 train_rate= 89.00% test_rate= 89.48% train_err= 0.37505 test_err= 0.38100\n",
      " 21 train_rate= 97.74% test_rate= 96.28% train_err= 0.06928 test_err= 0.13190\n",
      " 22 train_rate= 98.43% test_rate= 96.74% train_err= 0.04756 test_err= 0.12362\n",
      " 23 train_rate= 98.84% test_rate= 97.12% train_err= 0.03378 test_err= 0.11195\n",
      " 24 train_rate= 99.08% test_rate= 97.26% train_err= 0.02887 test_err= 0.11509\n",
      " 25 train_rate= 98.89% test_rate= 96.83% train_err= 0.03163 test_err= 0.13144\n",
      " 26 train_rate= 99.35% test_rate= 97.39% train_err= 0.01934 test_err= 0.11684\n",
      " 27 train_rate= 99.39% test_rate= 97.51% train_err= 0.01776 test_err= 0.11949\n",
      " 28 train_rate= 99.38% test_rate= 97.30% train_err= 0.01829 test_err= 0.12460\n",
      " 29 train_rate= 99.30% test_rate= 97.21% train_err= 0.02045 test_err= 0.14220\n",
      " 30 train_rate= 99.53% test_rate= 97.28% train_err= 0.01412 test_err= 0.13220\n",
      " 31 train_rate= 99.36% test_rate= 97.45% train_err= 0.01863 test_err= 0.13757\n",
      " 32 train_rate= 99.56% test_rate= 97.38% train_err= 0.01343 test_err= 0.14049\n",
      " 33 train_rate= 99.47% test_rate= 97.35% train_err= 0.01522 test_err= 0.13790\n",
      " 34 train_rate= 98.58% test_rate= 96.58% train_err= 0.04993 test_err= 0.21271\n",
      " 35 train_rate= 99.50% test_rate= 97.20% train_err= 0.01430 test_err= 0.15324\n",
      " 36 train_rate= 99.41% test_rate= 97.28% train_err= 0.01835 test_err= 0.16087\n",
      " 37 train_rate= 99.56% test_rate= 97.17% train_err= 0.01357 test_err= 0.15641\n",
      " 38 train_rate= 99.56% test_rate= 97.18% train_err= 0.01332 test_err= 0.16937\n",
      " 39 train_rate= 99.67% test_rate= 97.32% train_err= 0.01012 test_err= 0.15513\n",
      " 40 train_rate= 99.76% test_rate= 97.41% train_err= 0.00703 test_err= 0.14755\n",
      " 41 train_rate= 99.57% test_rate= 97.31% train_err= 0.01259 test_err= 0.16128\n",
      " 42 train_rate= 99.80% test_rate= 97.33% train_err= 0.00657 test_err= 0.15846\n",
      " 43 train_rate= 99.82% test_rate= 97.47% train_err= 0.00550 test_err= 0.15756\n",
      " 44 train_rate= 99.87% test_rate= 97.54% train_err= 0.00421 test_err= 0.15987\n",
      " 45 train_rate= 99.94% test_rate= 97.69% train_err= 0.00223 test_err= 0.14999\n",
      " 46 train_rate= 99.87% test_rate= 97.34% train_err= 0.00433 test_err= 0.16287\n",
      " 47 train_rate= 99.99% test_rate= 97.68% train_err= 0.00079 test_err= 0.15072\n",
      " 48 train_rate=100.00% test_rate= 97.68% train_err= 0.00040 test_err= 0.15013\n",
      " 49 train_rate=100.00% test_rate= 97.68% train_err= 0.00032 test_err= 0.15198\n",
      " 50 train_rate=100.00% test_rate= 97.69% train_err= 0.00028 test_err= 0.15306\n"
     ]
    }
   ],
   "source": [
    "# 画像のスケールを0-1に正規化\n",
    "nx_train = x_train/255\n",
    "nx_test = x_test/255\n",
    "\n",
    "# dk 第k層の次元を設定 \n",
    "d0 = nx_train.shape[1]\n",
    "d1 = 100\n",
    "d2 = 50\n",
    "d3 = 10\n",
    "\n",
    "np.random.seed(8)\n",
    "W1 = np.random.rand(d0, d1) * 0.2 -0.1\n",
    "W2 = np.random.rand(d1, d2) * 0.2 -0.1\n",
    "W3 = np.random.rand(d2, d3) * 0.2 -0.1\n",
    "\n",
    "# historyにこれまでの学習を記憶させるために、重みと同じ形状の零行列で初期化\n",
    "history_W1 = np.zeros_like(W1)\n",
    "history_W2 = np.zeros_like(W2)\n",
    "history_W3 = np.zeros_like(W3)\n",
    "\n",
    "b1 = np.zeros(d1)\n",
    "b2 = np.zeros(d2)\n",
    "b3 = np.zeros(d3)\n",
    "lr = 0.5\n",
    "\n",
    "batch_size = 100\n",
    "epoch = 50\n",
    "y_train = predict(nx_train, W1, b1, W2, b2, W3, b3)\n",
    "y_test = predict(nx_test, W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "# メソッドの実装はページ上部を参照\n",
    "history_W1 = momentum_decay(history_W1, W1, lr)\n",
    "history_W2 = momentum_decay(history_W2, W2, lr)\n",
    "history_W3 = momentum_decay(history_W3, W3, lr)\n",
    "\n",
    "train_rate, train_err = accuracy_rate(y_train, t_train), cross_entropy_error(y_train, t_train)\n",
    "test_rate, test_err = accuracy_rate(y_test, t_test), cross_entropy_error(y_test, t_test)\n",
    "print(\"{0:3d} train_rate={1:6.2f}% test_rate={2:6.2f}% train_err={3:8.5f} test_err={4:8.5f}\".format((0), train_rate*100, test_rate*100, train_err, test_err))\n",
    "for i in range(epoch):\n",
    "    for j in range(0,nx_train.shape[0], batch_size):\n",
    "        W1, b1, W2, b2, W3, b3, history_W1, history_W2, history_W3 = Momentum_learn(nx_train[j:j+batch_size], t_train[j:j+batch_size], W1, b1, W2, b2, W3, b3, history_W1, history_W2, history_W3, lr)\n",
    "    y_train = predict(nx_train, W1, b1, W2, b2, W3, b3)\n",
    "    y_test = predict(nx_test, W1, b1, W2, b2, W3, b3)\n",
    "    train_rate, train_err = accuracy_rate(y_train, t_train), cross_entropy_error(y_train, t_train)\n",
    "    test_rate, test_err = accuracy_rate(y_test, t_test), cross_entropy_error(y_test, t_test)\n",
    "    print(\"{0:3d} train_rate={1:6.2f}% test_rate={2:6.2f}% train_err={3:8.5f} test_err={4:8.5f}\".format((i+1), train_rate*100, test_rate*100, train_err, test_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
